{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/actividades-de-aprendizaje-A01373377/blob/main/A01373377_Data_Analysis_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 1"
      ],
      "metadata": {
        "id": "tl_6VjMdkK2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Los objetivos de este Módulo 1 son:\n",
        "\n",
        "\n",
        "*   Caso de estudios sobre análisis de precio de vehículos usando Python\n",
        "\n",
        "La data está en todos lados\n",
        "\n",
        "**El Análisis de Datos nos ayuda a responder preguntas sobre la data por medio de:**\n",
        "\n",
        "1) Descubrir información útil\n",
        "\n",
        "2) Con esa información útil resolver las pregunatas del problema\n",
        "\n",
        "3) Predecir el futuro o lo desconocido\n",
        "\n",
        "Importar formatos en csv format es bastante más sencillo\n",
        "\n",
        "**Tipos de librerías:**\n",
        "\n",
        "1) Science Computing Libraries:\n",
        "\n",
        "*  Pandas, Data structures & tools \n",
        "*  NumPy, Arrays & matrices\n",
        "*  SciPy, Integrals, solving differential equations, optimization\n",
        "\n",
        "2) Visualization Librearies\n",
        "\n",
        "*  Matplotlib, plots & graphs, most popular\n",
        "*  Seaborn, plots: heat maps, time series, violin plots\n",
        "\n",
        "3) Algorithmic libraries:\n",
        "\n",
        "*  Scikit-learn, Machine Learning: regression, classification, clustering, etc.\n",
        "*  Statsmodels, Explore data, estimate statistical models, and perform statistical tests.\n"
      ],
      "metadata": {
        "id": "B3vCue0ElQAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84o9K70XkHHi"
      },
      "outputs": [],
      "source": [
        "#importar un csv\n",
        "import pandas as pd\n",
        "url = \"https:\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df = pd.read_csv(url, header = None) #if the data used has no column headers we need to specify\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imprimiendo el dataframe\n",
        "df #no se recomienda para dataframes largos\n",
        "df.head(n) #muestras los primeros n renglones del dataframe\n",
        "df.tail(n) #muestras los últimos n renglones del dataframe"
      ],
      "metadata": {
        "id": "pmeS2RZc0Ynp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#para poder headears primero hacemos una lista con los nombres de los headers\n",
        "df.columns = nombre de la lista"
      ],
      "metadata": {
        "id": "t_GSQ3xv05KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#para guardar los cambios realizados a los dataframes\n",
        "path = \"ruta en tu pc\"\n",
        "df.to_csv(path)"
      ],
      "metadata": {
        "id": "3AJ7bpZG1OUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saber el tipo de valores del df\n",
        "df.types"
      ],
      "metadata": {
        "id": "n6JNgLmSMh9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# regresa el resumen estadístico, desviaciones estandar., promedio, número de valores\n",
        "df.describe()\n",
        "df.describe(include=\"all\") #nos incluye los datos de todas las columnas"
      ],
      "metadata": {
        "id": "4udKrzR0MokE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab"
      ],
      "metadata": {
        "id": "-vPgkj4WPpzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Adquisition\n",
        "\n",
        "There are various formats for a dataset: .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.\n",
        "In this section, you will learn how to load a dataset into our Jupyter Notebook.\n",
        "\n",
        "In our case, the Automobile Dataset is an online source, and it is in a CSV (comma separated value) format. Let's use this dataset as an example to practice data reading.\n",
        "\n",
        "Data source: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\n",
        "Data type: csv\n",
        "The Pandas Library is a useful tool that enables us to read various datasets into a dataframe; our Jupyter notebook platforms have a built-in Pandas Library so that all we need to do is import Pandas without installing.\n",
        "you are running the lab in your browser, so we will install the libraries using piplite"
      ],
      "metadata": {
        "id": "uk5irlSYQ4n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA ADQUISITION\n",
        "\n",
        "#you are running the lab in your  browser, so we will install the libraries using ``piplite``\n",
        "import piplite\n",
        "import micropip\n",
        "await piplite.install(['pandas'])\n",
        "await piplite.install(['matplotlib'])\n",
        "await piplite.install(['scipy'])\n",
        "await piplite.install(['seaborn'])\n",
        "await micropip.install(['ipywidgets'],keep_going=True)\n",
        "await micropip.install(['tqdm'],keep_going=True)"
      ],
      "metadata": {
        "id": "LuGfF_6XM9Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install specific version of libraries used in  lab\n",
        "#! mamba install pandas==1.3.3  -y\n",
        "#! mamba install numpy=1.21.2 -y"
      ],
      "metadata": {
        "id": "uyPUe150Qbvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Dsg_iGOiQ1P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will download the dataset into your browser \n",
        "\n",
        "from pyodide.http import pyfetch\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())"
      ],
      "metadata": {
        "id": "u7am0wJaQ3eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data\n",
        "\n",
        "There are various formats for a dataset: .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.\n",
        "In this section, you will learn how to load a dataset into our Jupyter Notebook.\n",
        "\n",
        "In our case, the Automobile Dataset is an online source, and it is in a CSV (comma separated value) format. Let's use this dataset as an example to practice data reading.\n",
        "\n",
        "Data source: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\n",
        "Data type: csv\n",
        "The Pandas Library is a useful tool that enables us to read various datasets into a dataframe; our Jupyter notebook platforms have a built-in Pandas Library so that all we need to do is import Pandas without installing.\n",
        "you are running the lab in your browser, so we will install the libraries using piplite"
      ],
      "metadata": {
        "id": "G4FxgmcTQ9Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\""
      ],
      "metadata": {
        "id": "bV09nds0RbZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you will need to download the dataset; if you are running locally, please comment out the following \n",
        "await download(path, \"auto.csv\")\n",
        "path=\"auto.csv\""
      ],
      "metadata": {
        "id": "9gFba-AkRjoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Read the online file by the URL provides above, and assign it to variable \"df\"\n",
        "\n",
        "df = pd.read_csv(path, header=None)"
      ],
      "metadata": {
        "id": "eFGmTd0uRlFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the first 5 rows using dataframe.head() method\n",
        "print(\"The first 5 rows of the dataframe\") \n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "SzDFsMQ-Rm0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 1\n",
        "#Check the bottom 10 rows of data frame \"df\".\n",
        "df.tail(10)"
      ],
      "metadata": {
        "id": "J68POTV3Sv8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Headers\n",
        "Take a look at our dataset. Pandas automatically set the header with an integer starting from 0.\n",
        "\n",
        "To better describe our data, we can introduce a header. This information is available at: https://archive.ics.uci.edu/ml/datasets/Automobile.\n",
        "\n",
        "Thus, we have to add headers manually.\n",
        "\n",
        "First, we create a list \"headers\" that include all column names in order. Then, we use dataframe.columns = headers to replace the headers with the list we created."
      ],
      "metadata": {
        "id": "GUcVn1ZbRpyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create headers list\n",
        "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
        "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
        "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
        "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n",
        "print(\"headers\\n\", headers)"
      ],
      "metadata": {
        "id": "-SQr5poYSKCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = headers\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "r4HbBliaSesH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.replace('?',np.NaN)\n"
      ],
      "metadata": {
        "id": "p1SqJWgrSg3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df1.dropna(subset=[\"price\"], axis=0)\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "jkJpbjJBShZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2\n",
        "# Find the name of the columns of the dataframe.\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "qVSP2P6RSqUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Dataset\n",
        "Correspondingly, Pandas enables us to save the dataset to csv. By using the dataframe.to_csv() method, you can add the file path and name along with quotation marks in the brackets.\n",
        "\n",
        "For example, if you would save the dataframe df as automobile.csv to your local machine, you may use the syntax below, where index = False means the row names will not be written."
      ],
      "metadata": {
        "id": "QiQg16vuS7NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"automobile.csv\", index=False)"
      ],
      "metadata": {
        "id": "KZXxE7UOTBr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Insight of Dataset\n",
        "\n",
        "After reading data into Pandas dataframe, it is time for us to explore the dataset.\n",
        "There are several ways to obtain essential insights of the data to help us better understand our dataset.\n",
        "\n",
        "**Data Types**\n",
        "\n",
        "Data has a variety of types.\n",
        "The main types stored in Pandas dataframes are object, float, int, bool and datetime64. In order to better learn about each attribute, it is always good for us to know the data type of each column. In Pandas:"
      ],
      "metadata": {
        "id": "NJRnH3F9TSf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "qReSGYJCTZZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data type of data frame \"df\" by .dtypes\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "SFMlllfNTbCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe**"
      ],
      "metadata": {
        "id": "O5-s6yZ1Touv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.describe()"
      ],
      "metadata": {
        "id": "a5mcciFITlbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "HD3aYwLZTnjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe all the columns in \"df\" \n",
        "df.describe(include = \"all\")"
      ],
      "metadata": {
        "id": "gJjJ4IbGTvST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Info"
      ],
      "metadata": {
        "id": "QxTizvC6Tygz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.info()"
      ],
      "metadata": {
        "id": "7M9Sn3PrTz__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the info of \"df\"\n",
        "df.info()"
      ],
      "metadata": {
        "id": "3AhPusVGT1Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graded Review Questions"
      ],
      "metadata": {
        "id": "zZEDT4ZOUcV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "What does CSV stand for?\n",
        "\n",
        "\n",
        "Comma-separated values"
      ],
      "metadata": {
        "id": "6w7nYZotVr7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "In the data set, which of the following represents an attribute or feature?\n",
        "\n",
        "Column"
      ],
      "metadata": {
        "id": "YgP0iHChV1uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "\n",
        "1/1 point (graded)\n",
        "\n",
        "What is the name of what we want to predict?\n",
        "\n",
        "Target"
      ],
      "metadata": {
        "id": "bMXkKe5GV4dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "What is the command to display the first five rows of a dataframe df?\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5LMHgcv2WFvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "What command do you use to get the data type of each row of the dataframe df?\n",
        "\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "cS0-LIDOWM_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "How do you get a statistical summary of a dataframe df?\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "pT1g1_adW1Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "If you use the method describe() without changing any of the arguments, you will get a statistical summary of all the columns of type \"object\".\n",
        "\n",
        "False"
      ],
      "metadata": {
        "id": "-_Tim-dFW6rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 2"
      ],
      "metadata": {
        "id": "_OQO9NsCulLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los objetivos de aprendizaje de este módulo son:\n",
        "\n",
        "*  Identificación y manejo de datos perdidos \n",
        "*  Formateo de datos\n",
        "*  Normalización de los datos, técnicas de centering y scaling\n",
        "*  Agrupamiento de datos\n",
        "*  Transformar valores categóricos en variables numéricas\n",
        "\n"
      ],
      "metadata": {
        "id": "JBw-tYTVvd6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab"
      ],
      "metadata": {
        "id": "6OO3bHqvBpwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the purpose of data wrangling?\n",
        "Data wrangling is the process of converting data from the initial format to a format that may be better for analysis.\n",
        "\n",
        "What is the fuel consumption (L/100k) rate for the diesel car?\n",
        "Import data\n",
        "You can find the \"Automobile Dataset\" from the following link: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data. We will be using this dataset throughout this course.\n",
        "\n",
        "Import pandas\n",
        "you are running the lab in your browser, so we will install the libraries using piplite"
      ],
      "metadata": {
        "id": "2FaWV1E9BsVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import piplite\n",
        "await piplite.install(['pandas'])\n",
        "await piplite.install(['matplotlib'])"
      ],
      "metadata": {
        "id": "L0AeQo4UBtkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you run the lab locally using Anaconda, you can load the correct library and versions by uncommenting the following:\n",
        "#install specific version of libraries used in lab\n",
        "#! mamba install pandas==1.3.3\n",
        "#! mamba install numpy=1.21.2"
      ],
      "metadata": {
        "id": "ixCclxWeBx9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "XfelSxPZByeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will download the dataset into your browser \n",
        "from pyodide.http import pyfetch\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())"
      ],
      "metadata": {
        "id": "JKQJHY3kB2hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset from the URL and adding the related headers¶\n",
        "First, we assign the URL of the dataset to \"filename\".\n",
        "\n",
        "This dataset was hosted on IBM Cloud object. Click HERE for free storage."
      ],
      "metadata": {
        "id": "giDJn2AnB5Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\""
      ],
      "metadata": {
        "id": "9qpS0Ne1B55b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we create a Python list headers containing name of headers."
      ],
      "metadata": {
        "id": "xaMvM69eB95V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
        "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
        "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
        "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]"
      ],
      "metadata": {
        "id": "nKLQd7DPB7g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you will need to download the dataset; if you are running locally, please comment out the following"
      ],
      "metadata": {
        "id": "bD88FX-JCB72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await download(filename, \"auto.csv\")\n",
        "filename=\"auto.csv\""
      ],
      "metadata": {
        "id": "yk5wSJzjCG1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Pandas method read_csv() to load the data from the web address. Set the parameter \"names\" equal to the Python list \"headers\"."
      ],
      "metadata": {
        "id": "yX6WcqPICIRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filename, names = headers)"
      ],
      "metadata": {
        "id": "dFoDyCOyCJ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Pandas method read_csv() to load the data from the web address. Set the parameter \"names\" equal to the Python list \"headers\"."
      ],
      "metadata": {
        "id": "2Fz0RWNRCL-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filename, names = headers)"
      ],
      "metadata": {
        "id": "H75eblkQCODA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the method head() to display the first five rows of the dataframe."
      ],
      "metadata": {
        "id": "T0VXOpNvCP2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see what the data set looks like, we'll use the head() method.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8cLabKwNCccc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, several question marks appeared in the dataframe; those are missing values which may hinder our further analysis.\n",
        "\n",
        "So, how do we identify all those missing values and deal with them?\n",
        "How to work with missing data?\n",
        "\n",
        "Steps for working with missing data:\n",
        "\n",
        "Identify missing data\n",
        "Deal with missing data\n",
        "Correct data format\n",
        "Identify and handle missing values\n",
        "Identify missing values\n",
        "Convert \"?\" to NaN\n",
        "In the car dataset, missing data comes with the question mark \"?\". We replace \"?\" with NaN (Not a Number), Python's default missing value marker for reasons of computational speed and convenience. Here we use the function:\n",
        ".replace(A, B, inplace = True) \n",
        "to replace A by B."
      ],
      "metadata": {
        "id": "7mlHp8t5Ceh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# replace \"?\" to NaN\n",
        "df.replace(\"?\", np.nan, inplace = True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "09FLH0WCCfOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating for Missing Data\n",
        "The missing values are converted by default. We use the following functions to identify these missing values. There are two methods to detect missing data:\n",
        "\n",
        ".isnull()\n",
        "\n",
        ".notnull()\n",
        "\n",
        "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data."
      ],
      "metadata": {
        "id": "uUUmMZBIChOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = df.isnull()\n",
        "missing_data.head(5)"
      ],
      "metadata": {
        "id": "hWxOAdvpCkhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"True\" means the value is a missing value while \"False\" means the value is not a missing value.\n",
        "\n",
        "Count missing values in each column\n",
        "Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value and \"False\" means the value is present in the dataset. In the body of the for loop the method \".value_counts()\" counts the number of \"True\" values."
      ],
      "metadata": {
        "id": "OyezK7-pCmyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in missing_data.columns.values.tolist():\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")  "
      ],
      "metadata": {
        "id": "6FKpzmKLCrXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the summary above, each column has 205 rows of data and seven of the columns containing missing data:\n",
        "\n",
        "\"normalized-losses\": 41 missing data\n",
        "\n",
        "\"num-of-doors\": 2 missing data\n",
        "\n",
        "\"bore\": 4 missing data\n",
        "\n",
        "\"stroke\" : 4 missing data\n",
        "\n",
        "\"horsepower\": 2 missing data\n",
        "\n",
        "\"peak-rpm\": 2 missing data\n",
        "\n",
        "\"price\": 4 missing data\n",
        "\n",
        "Deal with missing data\n",
        "\n",
        "How to deal with missing data?\n",
        "Drop data\n",
        "\n",
        "a. Drop the whole row\n",
        "\n",
        "b. Drop the whole column\n",
        "\n",
        "Replace data\n",
        "\n",
        "a. Replace it by mean\n",
        "\n",
        "b. Replace it by frequency\n",
        "\n",
        "c. Replace it based on other functions\n",
        "\n",
        "Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:\n",
        "\n",
        "Replace by mean:\n",
        "\n",
        "\"normalized-losses\": 41 missing data, replace them with mean\n",
        "\n",
        "\"stroke\": 4 missing data, replace them with mean\n",
        "\n",
        "\"bore\": 4 missing data, replace them with mean\n",
        "\n",
        "\"horsepower\": 2 missing data, replace them with mean\n",
        "\n",
        "\n",
        "\"peak-rpm\": 2 missing data, replace them with mean\n",
        "\n",
        "Replace by frequency:\n",
        "\n",
        "\"num-of-doors\": 2 missing data, replace them with \"four\".\n",
        "Reason: 84% sedans is four doors. Since four doors is most frequent, it is most likely to occur\n",
        "Drop the whole row:\n",
        "\n",
        "\"price\": 4 missing data, simply delete the whole row\n",
        "Reason: price is what we want to predict. Any data entry without price data cannot be used for prediction; therefore any row now without price data is not useful to us\n",
        "Calculate the mean value for the \"normalized-losses\" column ¶"
      ],
      "metadata": {
        "id": "evV4nZrVCsBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_norm_loss = df[\"normalized-losses\"].astype(\"float\").mean(axis=0)\n",
        "print(\"Average of normalized-losses:\", avg_norm_loss)"
      ],
      "metadata": {
        "id": "ntNSJBJEC6tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace \"NaN\" with mean value in \"normalized-losses\" column"
      ],
      "metadata": {
        "id": "A_GwFVcmC-GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"normalized-losses\"].replace(np.nan, avg_norm_loss, inplace=True)"
      ],
      "metadata": {
        "id": "xzItRhooC_k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the mean value for the \"bore\" column"
      ],
      "metadata": {
        "id": "-SRYel9YDBjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bore=df['bore'].astype('float').mean(axis=0)\n",
        "print(\"Average of bore:\", avg_bore)"
      ],
      "metadata": {
        "id": "qmv-RqTWDDuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace \"NaN\" with the mean value in the \"bore\" column"
      ],
      "metadata": {
        "id": "l0IXWiYBDFxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"bore\"].replace(np.nan, avg_bore, inplace=True)"
      ],
      "metadata": {
        "id": "cE2-2P_XDGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct data format\n",
        "We are almost there!\n",
        "The last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other).\n",
        "\n",
        "In Pandas, we use:\n",
        "\n",
        ".dtype() to check the data type\n",
        "\n",
        ".astype() to change the data type\n",
        "\n",
        "Let's list the data types for each column"
      ],
      "metadata": {
        "id": "dw5deydVDLyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "gwzde4dfDO05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, some columns are not of the correct data type. Numerical variables should have type 'float' or 'int', and variables with strings such as categories should have type 'object'. For example, 'bore' and 'stroke' variables are numerical values that describe the engines, so we should expect them to be of the type 'float' or 'int'; however, they are shown as type 'object'. We have to convert data types into a proper format for each column using the \"astype()\" method.\n",
        "\n",
        "Convert data types to proper format"
      ],
      "metadata": {
        "id": "ytzqOzyEDaqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"bore\", \"stroke\"]] = df[[\"bore\", \"stroke\"]].astype(\"float\")\n",
        "df[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")\n",
        "df[[\"price\"]] = df[[\"price\"]].astype(\"float\")\n",
        "df[[\"peak-rpm\"]] = df[[\"peak-rpm\"]].astype(\"float\")"
      ],
      "metadata": {
        "id": "fEL7Tk59DZrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us list the columns after the conversion"
      ],
      "metadata": {
        "id": "tZ7RziBlDefB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "PHoTBpOJDe5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Standardization\n",
        "Data is usually collected from different agencies in different formats. (Data standardization is also a term for a particular type of data normalization where we subtract the mean and divide by the standard deviation.)\n",
        "\n",
        "What is standardization?\n",
        "\n",
        "Standardization is the process of transforming data into a common format, allowing the researcher to make the meaningful comparison.\n",
        "\n",
        "Example\n",
        "\n",
        "Transform mpg to L/100km:\n",
        "\n",
        "In our dataset, the fuel consumption columns \"city-mpg\" and \"highway-mpg\" are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accepts the fuel consumption with L/100km standard.\n",
        "\n",
        "We will need to apply data transformation to transform mpg into L/100km.\n",
        "\n",
        "The formula for unit conversion is:\n",
        "\n",
        "L/100km = 235 / mpg\n",
        "\n",
        "We can do many mathematical operations directly in Pandas."
      ],
      "metadata": {
        "id": "EsuqREntDimi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QolhUYTCDjYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert mpg to L/100km by mathematical operation (235 divided by mpg)\n",
        "df['city-L/100km'] = 235/df[\"city-mpg\"]\n",
        "\n",
        "# check your transformed data \n",
        "df.head()"
      ],
      "metadata": {
        "id": "MZwCKu6KDlff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization\n",
        "Why normalization?\n",
        "\n",
        "Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling the variable so the variable values range from 0 to 1.\n",
        "\n",
        "Example\n",
        "\n",
        "To demonstrate normalization, let's say we want to scale the columns \"length\", \"width\" and \"height\".\n",
        "\n",
        "Target: would like to normalize those variables so their value ranges from 0 to 1\n",
        "\n",
        "Approach: replace original value by (original value)/(maximum value)"
      ],
      "metadata": {
        "id": "ARuAa2ijDqUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace (original value) by (original value)/(maximum value)\n",
        "df['length'] = df['length']/df['length'].max()\n",
        "df['width'] = df['width']/df['width'].max()"
      ],
      "metadata": {
        "id": "LrL-BX9iDq12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binning\n",
        "Why binning?\n",
        "Binning is a process of transforming continuous numerical variables into discrete categorical 'bins' for grouped analysis.\n",
        "\n",
        "Example:\n",
        "\n",
        "In our dataset, \"horsepower\" is a real valued variable ranging from 48 to 288 and it has 59 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three ‘bins' to simplify analysis?\n",
        "\n",
        "We will use the pandas method 'cut' to segment the 'horsepower' column into 3 bins.\n",
        "\n",
        "Example of Binning Data In Pandas\n",
        "Convert data to correct format:"
      ],
      "metadata": {
        "id": "a1J3wdAYDtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"horsepower\"]=df[\"horsepower\"].astype(int, copy=True)"
      ],
      "metadata": {
        "id": "cw_9d3bjDt9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the histogram of horsepower to see what the distribution of horsepower looks like."
      ],
      "metadata": {
        "id": "Yb-dgsRbDxOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plt\n",
        "from matplotlib import pyplot\n",
        "plt.pyplot.hist(df[\"horsepower\"])\n",
        "\n",
        "# set x/y labels and plot title\n",
        "plt.pyplot.xlabel(\"horsepower\")\n",
        "plt.pyplot.ylabel(\"count\")\n",
        "plt.pyplot.title(\"horsepower bins\")"
      ],
      "metadata": {
        "id": "b5epJcH_DxxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like 3 bins of equal size bandwidth so we use numpy's linspace(start_value, end_value, numbers_generated function.\n",
        "\n",
        "Since we want to include the minimum value of horsepower, we want to set start_value = min(df[\"horsepower\"]).\n",
        "\n",
        "Since we want to include the maximum value of horsepower, we want to set end_value = max(df[\"horsepower\"]).\n",
        "\n",
        "Since we are building 3 bins of equal length, there should be 4 dividers, so numbers_generated = 4.\n",
        "\n",
        "We build a bin array with a minimum value to a maximum value by using the bandwidth calculated above. The values will determine when one bin ends and another begins."
      ],
      "metadata": {
        "id": "U3szhuXbDzVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = np.linspace(min(df[\"horsepower\"]), max(df[\"horsepower\"]), 4)\n",
        "bins"
      ],
      "metadata": {
        "id": "AQ7nIa6wD1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set group names:"
      ],
      "metadata": {
        "id": "3Armopi9D4R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_names = ['Low', 'Medium', 'High']"
      ],
      "metadata": {
        "id": "vFj7qNScD8cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the function \"cut\" to determine what each value of df['horsepower'] belongs to."
      ],
      "metadata": {
        "id": "BD5zFfnBD-OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )\n",
        "df[['horsepower','horsepower-binned']].head(20)"
      ],
      "metadata": {
        "id": "xUc6XdmTEAQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the number of vehicles in each bin:"
      ],
      "metadata": {
        "id": "5C0_dUnjEB9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"horsepower-binned\"].value_counts()"
      ],
      "metadata": {
        "id": "Cq9Zq_DHEJug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the distribution of each bin:"
      ],
      "metadata": {
        "id": "o_Q9nWOoEII2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plt\n",
        "from matplotlib import pyplot\n",
        "pyplot.bar(group_names, df[\"horsepower-binned\"].value_counts())\n",
        "\n",
        "# set x/y labels and plot title\n",
        "plt.pyplot.xlabel(\"horsepower\")\n",
        "plt.pyplot.ylabel(\"count\")\n",
        "plt.pyplot.title(\"horsepower bins\")"
      ],
      "metadata": {
        "id": "CKuG0I-iELJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bins Visualization\n",
        "\n",
        "Normally, a histogram is used to visualize the distribution of bins we created above."
      ],
      "metadata": {
        "id": "_fWUal16ENhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "# draw historgram of attribute \"horsepower\" with bins = 3\n",
        "plt.pyplot.hist(df[\"horsepower\"], bins = 3)\n",
        "\n",
        "# set x/y labels and plot title\n",
        "plt.pyplot.xlabel(\"horsepower\")\n",
        "plt.pyplot.ylabel(\"count\")\n",
        "plt.pyplot.title(\"horsepower bins\")"
      ],
      "metadata": {
        "id": "hqfzdUdvEQWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicator Variable (or Dummy Variable)\n",
        "What is an indicator variable?\n",
        "An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning.\n",
        "\n",
        "Why we use indicator variables?\n",
        "\n",
        "We use indicator variables so we can use categorical variables for regression analysis in the later modules.\n",
        "\n",
        "Example\n",
        "We see the column \"fuel-type\" has two unique values: \"gas\" or \"diesel\". Regression doesn't understand words, only numbers. To use this attribute in regression analysis, we convert \"fuel-type\" to indicator variables.\n",
        "\n",
        "We will use pandas' method 'get_dummies' to assign numerical values to different categories of fuel type."
      ],
      "metadata": {
        "id": "EtDd6VZ9EUr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "LX4NlxISEVZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the indicator variables and assign it to data frame \"dummy_variable_1\":"
      ],
      "metadata": {
        "id": "FLItUuYaEXyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_variable_1 = pd.get_dummies(df[\"fuel-type\"])\n",
        "dummy_variable_1.head()"
      ],
      "metadata": {
        "id": "i_NkeN-tEYW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the column names for clarity:"
      ],
      "metadata": {
        "id": "b4kSB8jvEZ4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_variable_1.rename(columns={'gas':'fuel-type-gas', 'diesel':'fuel-type-diesel'}, inplace=True)\n",
        "dummy_variable_1.head()"
      ],
      "metadata": {
        "id": "MrYf1B5VEcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataframe, column 'fuel-type' has values for 'gas' and 'diesel' as 0s and 1s now."
      ],
      "metadata": {
        "id": "nfR8K5WhEbzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge data frame \"df\" and \"dummy_variable_1\" \n",
        "df = pd.concat([df, dummy_variable_1], axis=1)\n",
        "\n",
        "# drop original column \"fuel-type\" from \"df\"\n",
        "df.drop(\"fuel-type\", axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "vNKJSGOKEgyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eLkx7KNcEiEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last two columns are now the indicator variable representation of the fuel-type variable. They're all 0s and 1s now."
      ],
      "metadata": {
        "id": "ZNeUux0iEkfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "\n",
        "1/1 point (graded)\n",
        "\n",
        "Consider the dataframe df. What is the result of the following operation: df['symbolling'] = df['symbolling'] + 1?\n",
        "\n",
        "\n",
        "Answer: Every element in the column \"symbolling\" will increase by one."
      ],
      "metadata": {
        "id": "hDvprn0SEyAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "Consider the dataframe df. What does the command df.rename(columns={'a':'b'}) change about the dataframe df?\n",
        "\n",
        "Answer: Nothing. You must set the parameter \"inplace = True\"."
      ],
      "metadata": {
        "id": "QFUgPZIHE82v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "Consider the dataframe \"df\". What is the result of the following operation df['price'] = df['price'].astype(int)?\n",
        "\n",
        "Answer: Convert or cast the column 'price' to an integer value."
      ],
      "metadata": {
        "id": "MNtWLBsUFSXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "Consider the column of the dataframe df['a']. The column has been standardized. What is the standard deviation of the values as a result of applying the following operation: df['a'].std()?\n",
        "\n",
        "Answer: 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UIZY2i5qFd1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 a)\n",
        "\n",
        "1 point possible (graded)\n",
        "\n",
        "Consider the column of the dataframe, df['Fuel'], with two values: 'gas' and' diesel'. What will be the name of the new columns pd.get_dumies(df['Fuel']) ?\n",
        "\n",
        "Answer: 'gas' and 'diesel'"
      ],
      "metadata": {
        "id": "_oXXjOArFngI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 b)\n",
        "\n",
        "1/1 point (graded)\n",
        "\n",
        "What are the values of the new columns from part 5a)?\n",
        "\n",
        "Answer: 1 and 0"
      ],
      "metadata": {
        "id": "GYlE3M1jGNR8"
      }
    }
  ]
}